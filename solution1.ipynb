{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#features extraction and encoding:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "#nltk for stopwords and tokenizer:\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "#visualization tools:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "\n",
    "#sparse matrix:\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "#regressor validation:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#regressors to test:\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev=pd.read_csv('dev.tsv',sep='\\t')\n",
    "df_eval=pd.read_csv('eval.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read me:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code is able to quickly reproduce the 0.859 public score submitted. If the line in section 1a is uncommented, then this code reproduce the 0.846 public score result. Output can be found in output_mlp.csv. In the solution 2 file can be found all the validation step, visualization code and all the material discussed in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_dev cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a\n",
    "df_dev=df_dev[df_dev['quality']>0]\n",
    "#[uncomment line below for 0.846 score]# \n",
    "# df_dev.drop_duplicates(subset=['description','quality'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attribute preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df_dev,df_eval],sort=False,ignore_index=True)   #we merge together train and eval set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "designation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prestige\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'ve\", '``', 'avon', 'could', 'diesis', 'doe', 'dy', 'f', 'forum', 'fuss', 'might', 'mus', 'must', \"n't\", 'need', 'onların', 'quantum', 'reed', 'serum', 'sha', 'st', 'wa', 'would', 'δ', 'δι', 'агар-чи', 'аз-баски', 'афташ', 'бале', 'баҳри', 'болои', 'валекин', 'вақте', 'вуҷуди', 'гар', 'гарчанде', 'даме', 'карда', 'кошки', 'куя', 'кӣ', 'магар', 'майлаш', 'модоме', 'нияти', 'онан', 'оре', 'рӯи', 'сар', 'тразе', 'хом', 'хуб', 'чаро', 'чи', 'чунон', 'ш', 'шарте', 'қадар', 'ҳай-ҳай', 'ҳамин', 'ҳатто', 'ҳо', 'ҳой-ҳой', 'ҳол', 'ҳолате', 'ӯим', 'कम', 'से', 'ἀλλ', '’'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "#text normalization step\n",
    "df['designation']=df['designation'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "        for t in word_tokenize(document):\n",
    "            t = t.strip()\n",
    "            lemma = self.lemmatizer.lemmatize(t)\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "\n",
    "\n",
    "lemmaTokenizer = LemmaTokenizer()\n",
    "list_sw=stopwords.words() + list(string.punctuation) + ['st.',\"'s\",'wine','vine','']\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer,stop_words=list_sw, use_idf=False, norm=False, binary=True)\n",
    "wpm = vectorizer.fit_transform(df['designation'].fillna(''))\n",
    "\n",
    "N = 5000\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)[:N]\n",
    "words = [ word for word, _ in freq ]  #we take the top N word\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "desig_words_df=wpm[:, np.array(mask)].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "winery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "df['winery']=df['winery'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "\n",
    "winery=df['winery'].value_counts()\n",
    "\n",
    "N_entries=2\n",
    "winery_mask=winery.values>=N_entries\n",
    "top_frequent_winery=winery[winery_mask].index\n",
    "\n",
    "df_winery_mask=df['winery'].isin(top_frequent_winery)\n",
    "df['tf_winery']=df['winery'][df_winery_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "geografical information: country, region1, province:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "df['province']=df['province'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "df['region_1']=df['region_1'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "province=df['province'].value_counts().index\n",
    "region_1=df['region_1'].value_counts().index\n",
    "common_value=np.intersect1d(province, region_1)\n",
    "df.loc[df['province']==df['region_1'],'region_1']=np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variety:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "df['variety']=df['variety'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "\n",
    "variety=df['variety'].value_counts()\n",
    "\n",
    "N_entries=7\n",
    "variety_mask=variety.values>=N_entries\n",
    "top_frequent_variety=variety[variety_mask].index\n",
    "\n",
    "df_variety_mask=df['variety'].isin(top_frequent_variety)\n",
    "df['tf_variety']=df['variety'][df_variety_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality=df['quality']\n",
    "df.drop(labels=['country','winery','variety','description','designation','quality','region_2'],axis=1,inplace=True)\n",
    "\n",
    "tresh=df_dev.shape[0]\n",
    "\n",
    "df_1h=pd.get_dummies(df,sparse=True)   #one hot encoding of the categorical attribute\n",
    "df_1h=hstack([df_1h,desig_words_df])\n",
    "df_1h=df_1h.tocsr()\n",
    "\n",
    "X_dev=df_1h[:tresh,:]\n",
    "y_dev=df_quality[:tresh]\n",
    "\n",
    "X_eval=df_1h[tresh:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(64,128,128,128,256,256,256,516,516,516,1024),\n",
    "                                random_state=42,\n",
    "                                verbose=True,\n",
    "                                early_stopping=True\n",
    "                               )\n",
    "                   \n",
    "reg.fit(X_dev, y_dev)\n",
    "y_pred = reg.predict(X_eval)\n",
    "\n",
    "pd.DataFrame(y_pred).to_csv(\"output_mlp.csv\",index_label=\"Id\", header=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
