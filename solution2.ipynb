{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#features extraction and encoding:\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "#nltk for stopwords and tokenizer:\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "#visualization tools:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import gridspec\n",
    "\n",
    "#sparse matrix:\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "#regressor validation:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#regressors to test:\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev=pd.read_csv('dev.tsv',sep='\\t')\n",
    "df_eval=pd.read_csv('eval.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read me:\n",
    "in order to reproduce the score submitted, the following block of code has to be executed:\n",
    " - 0.859 score: [1a] [2] [3] [4] [5] [6], [7]'Final encoding' section, [8]'best performer' section under 'final models attempt (and print out):' section\n",
    " - 0.859 score: [1a] [1b] [2] [3] [4] [5] [6] 'Final encoding' section, 'best performer' section under 'final models attempt (and print out):' section\n",
    " \n",
    "other sections contains snippet of code discussed in the report or used for the graph. Since most of them did "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data visualization (graph used for the report):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(8.5,5)})\n",
    "sns.set_style(\"ticks\") \n",
    "sns.set_context(\"paper\", font_scale=1.7) \n",
    "\n",
    "\n",
    "null_value=df_dev.isnull().sum().sort_values().values\n",
    "null_label=df_dev.isnull().sum().sort_values().index\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "graph=sns.barplot(null_value,null_label,palette=[\"C0\", \"C1\", \"C2\"],ax=ax)\n",
    "ax.set_xlim(1,90000)\n",
    "ax.set_xticks(np.arange(0,90000,10000))\n",
    "ax.set(xlabel='total number of null values')\n",
    "ax.set(ylabel='attributes')\n",
    "\n",
    "plt.grid()\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "#fig1.savefig(\"paper per report/grafici/nullValueCount.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(7,5)})\n",
    "sns.set_style(\"ticks\") \n",
    "sns.set_context(\"paper\", font_scale=1.7)    \n",
    "\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[0.6, 2], ) \n",
    "\n",
    "ax0 = plt.subplot(gs[0])\n",
    "g1=sns.boxplot(x=df_dev['quality'], width=0.4, ax=ax0)\n",
    "\n",
    "# remove the tick labels\n",
    "#g1.set(title='boxplot for the quality values')  # add a title\n",
    "#g1.set(xlabel='quality values')  # remove the axis label\n",
    "\n",
    "ax1 = plt.subplot(gs[1],sharex=ax0)\n",
    "g2=sns.histplot(x=df_dev['quality'],ax=ax1,bins=50)\n",
    "g2.set(xlabel='quality values')\n",
    "g2.set(ylabel='frequency')\n",
    "\n",
    "plt.subplots_adjust(hspace=.0)\n",
    "plt.grid()\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig1.savefig(\"paper per report/grafici/qualityCount2.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#text normalization sample\n",
    "string=df_dev.loc[92093:92190,'designation']\n",
    "normal=string.str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "print(string.values,normal.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "description wordcloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lemmaTokenizer = LemmaTokenizer()\n",
    "df['description']=df['description'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "\n",
    "list_sw=stopwords.words() + list(string.punctuation) + ['st.',\"'s\",'wine','vine','tannin','flavor','fruit']\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer,stop_words=list_sw, use_idf=False, norm=False, binary=False, max_features=100, ngram_range=(1,3))\n",
    "wpm = vectorizer.fit_transform(df['description'].fillna(''))\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "d = {}\n",
    "for a, x in freq:\n",
    "    d[a] = x\n",
    "\n",
    "Cloud = WordCloud(background_color=\"white\", max_words=100,width=800, height=400).generate_from_frequencies(d)\n",
    "\n",
    "plt.figure( figsize=(20,10) )\n",
    "plt.imshow(Cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "fig1.savefig(\"paper per report/grafici/worddesc.png\",bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df_dev preprocessing and cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1a]\n",
    "quality 0 entries can be considered as noise: probably this value is missing and 0 was used as std value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1a\n",
    "df_dev=df_dev[df_dev['quality']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see if there's some similar entries/duplicate values: let's check if we have:\n",
    " - duplicated description\n",
    " - entries with all attributes value in common (even quality), except the description\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_dev[df_dev.duplicated(['description','quality'],keep=False)].sort_values(by='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_dev[df_dev.duplicated(['country','designation','province','region_1','region_2','variety','winery','quality'],keep=False)].sort_values(by=['country','designation','province','region_1','region_2','variety','winery','quality'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1b] \n",
    "let's drop the duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1b\n",
    "df_dev.drop_duplicates(subset=['description','quality'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attribute preprocessing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "df=pd.concat([df_dev,df_eval],sort=False,ignore_index=True)   #we merge together train and eval set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "let's try to 1h encode the most frequent designation (at least N_entries). First, we normalize the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#discarded approach\n",
    "df['designation']=df['designation'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "designation=df['designation'].value_counts()\n",
    "\n",
    "N_entries=4\n",
    "desig_mask=designation.values>=N_entries\n",
    "top_frequent_desig=designation[desig_mask].index\n",
    "\n",
    "df_desig_mask=df['designation'].isin(top_frequent_desig)\n",
    "df['tf_desig']=df['designation'][df_desig_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3]\n",
    "let's 1he encode the top frequent word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prestige\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'ve\", '``', 'avon', 'could', 'diesis', 'doe', 'dy', 'f', 'forum', 'fuss', 'might', 'mus', 'must', \"n't\", 'need', 'onların', 'quantum', 'reed', 'serum', 'sha', 'st', 'wa', 'would', 'δ', 'δι', 'агар-чи', 'аз-баски', 'афташ', 'бале', 'баҳри', 'болои', 'валекин', 'вақте', 'вуҷуди', 'гар', 'гарчанде', 'даме', 'карда', 'кошки', 'куя', 'кӣ', 'магар', 'майлаш', 'модоме', 'нияти', 'онан', 'оре', 'рӯи', 'сар', 'тразе', 'хом', 'хуб', 'чаро', 'чи', 'чунон', 'ш', 'шарте', 'қадар', 'ҳай-ҳай', 'ҳамин', 'ҳатто', 'ҳо', 'ҳой-ҳой', 'ҳол', 'ҳолате', 'ӯим', 'कम', 'से', 'ἀλλ', '’'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "#text normalization step\n",
    "df['designation']=df['designation'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, document):\n",
    "        lemmas = []\n",
    "        for t in word_tokenize(document):\n",
    "            t = t.strip()\n",
    "            lemma = self.lemmatizer.lemmatize(t)\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "\n",
    "\n",
    "lemmaTokenizer = LemmaTokenizer()\n",
    "list_sw=stopwords.words() + list(string.punctuation) + ['st.',\"'s\",'wine','vine','']\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer,stop_words=list_sw, use_idf=False, norm=False, binary=True)\n",
    "wpm = vectorizer.fit_transform(df['designation'].fillna(''))\n",
    "\n",
    "N = 5000\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)[:N]\n",
    "words = [ word for word, _ in freq ]  #we take the top N word\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "desig_words_df=wpm[:, np.array(mask)].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "target encoding trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "encoder = TargetEncoder(cols=['designation'])\n",
    "\n",
    "dev_desig_te = encoder.fit_transform(df_dev['designation'],df_dev['quality'])\n",
    "eval_desig_te = encoder.transform(df_eval['designation'])\n",
    "\n",
    "\n",
    "df_te_desig=pd.concat([dev_desig_te,eval_desig_te],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### winery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [4] \n",
    "we 1h encode the winery that has at least N entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "df['winery']=df['winery'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "\n",
    "winery=df['winery'].value_counts()\n",
    "\n",
    "N_entries=2\n",
    "winery_mask=winery.values>=N_entries\n",
    "top_frequent_winery=winery[winery_mask].index\n",
    "\n",
    "df_winery_mask=df['winery'].isin(top_frequent_winery)\n",
    "df['tf_winery']=df['winery'][df_winery_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "target encoder trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#discarded approach\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "encoder = TargetEncoder(cols=['winery'])\n",
    "\n",
    "dev_winery_te = encoder.fit_transform(df_dev['winery'],df_dev['quality'])\n",
    "eval_winery_te = encoder.transform(df_eval['winery'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df_te_winery=pd.concat([dev_winery_te,eval_winery_te],ignore_index=True)\n",
    "df_te_winery=scaler.fit_transform(df_te_winery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### geografical information: country, region1, province"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check for rendundant values in province and region 1 attributes: if same values are found, we delete the value in region_1, so the encoder won't encode the same value two times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "df['province']=df['province'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "df['region_1']=df['region_1'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "province=df['province'].value_counts().index\n",
    "region_1=df['region_1'].value_counts().index\n",
    "common_value=np.intersect1d(province, region_1)\n",
    "df.loc[df['province']==df['region_1'],'region_1']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df[df['province']==df['region_1']] #query to check if all went well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variety:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "df['variety']=df['variety'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "\n",
    "variety=df['variety'].value_counts()\n",
    "\n",
    "N_entries=7\n",
    "variety_mask=variety.values>=N_entries\n",
    "top_frequent_variety=variety[variety_mask].index\n",
    "\n",
    "df_variety_mask=df['variety'].isin(top_frequent_variety)\n",
    "df['tf_variety']=df['variety'][df_variety_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf one hot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#discarded approach, poor results\n",
    "lemmaTokenizer = LemmaTokenizer()\n",
    "\n",
    "df['variety']=df['variety'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') #text normalization\n",
    "df['variety']=df['variety'].str.replace('-','  ')\n",
    "df['variety']=df['variety'].str.replace(' - ','  ')\n",
    "\n",
    "list_sw=stopwords.words() + list(string.punctuation) + ['st.',\"'s\",'wine','vine','-']\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer,stop_words=list_sw, use_idf=False, norm=False, binary=True)\n",
    "wpm = vectorizer.fit_transform(df['variety'].fillna(''))\n",
    "\n",
    "#here we don't need N because freq has a low number of features\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)\n",
    "words = [ word for word, _ in freq ]  #we take the top word\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "variety_words_df=wpm[:, np.array(mask)].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "All the approach tried below were discarded since they lead to worse results.\n",
    "\n",
    "we can try to use the description to predict the null value in the designation columns, in order to train better the regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_description=df[['description','designation','province','variety','winery']]\n",
    "\n",
    "predict_mask=df['designation'].isna()\n",
    "\n",
    "df_desc_dev=df_description[~predict_mask]\n",
    "df_desc_eval=df_description[predict_mask]\n",
    "\n",
    "index_desig_predict=df_desc_eval.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#normalization: df_dev\n",
    "df_desc_dev['designation']=df_desc_dev['designation'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') \n",
    "df_desc_dev['province']=df_desc_dev['province'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "df_desc_dev['winery']=df_desc_dev['winery'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "df_desc_dev['variety']=df_desc_dev['variety'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') \n",
    "\n",
    "df_desc_eval['designation']=df_desc_eval['designation'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') \n",
    "df_desc_eval['province']=df_desc_eval['province'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "df_desc_eval['winery']=df_desc_eval['winery'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "df_desc_eval['variety']=df_desc_eval['variety'].str.lower().str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "N=20\n",
    "topf_desig=df_desc_dev['designation'].value_counts()\n",
    "topN_desig=topf_desig[topf_desig > N].index\n",
    "top_f_desig_mask=df_desc_dev['designation'].isin(topN_desig)\n",
    "df_desc_dev=df_desc_dev[top_f_desig_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "lemmaTokenizer = LemmaTokenizer()\n",
    "list_sw=stopwords.words() + list(string.punctuation) + ['st.',\"'s\",'wine','vine','-']\n",
    "vectorizer = TfidfVectorizer(tokenizer=lemmaTokenizer,stop_words=list_sw, use_idf=False, norm=False, binary=True, ngram_range=(2,5),max_features=1000)\n",
    "\n",
    "vectorizer.fit(df_desc_dev['description'].fillna(''))\n",
    "wpm = vectorizer.transform(df_desc_dev['description'].fillna(''))\n",
    "wpm_eval = vectorizer.transform(df_desc_eval['description'].fillna(''))\n",
    "\n",
    "freq = sorted(zip(vectorizer.get_feature_names(), wpm.sum(axis=0).tolist()[0]),key=lambda x: x[1], reverse=True)\n",
    "words = [ word for word, _ in freq ]  #we take the top N word\n",
    "mask = [ w in words for w in vectorizer.get_feature_names() ]\n",
    "words_ = [ w for w in vectorizer.get_feature_names() if w in words ]\n",
    "\n",
    "description_words_df_desc =wpm[:, np.array(mask)].toarray()\n",
    "description_words_df_desc_eval =wpm_eval[:, np.array(mask)].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#we have to encode the labels of the designation, wich will be our target variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le =LabelEncoder()\n",
    "y_dev=le.fit_transform(df_desc_dev['designation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy.sparse import hstack, vstack\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(df_desc_dev[['province','variety','winery']])\n",
    "\n",
    "desc_1h_dev=enc.transform(df_desc_dev[['province','variety','winery']])\n",
    "desc_1h_eval=enc.transform(df_desc_eval[['province','variety','winery']])\n",
    "\n",
    "X_dev=hstack([description_words_df_desc,desc_1h_dev])\n",
    "X_eval=hstack([description_words_df_desc_eval,desc_1h_eval])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "validation of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_dev,y_dev, test_size= 0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_valid)\n",
    "\n",
    "\n",
    "acc=accuracy_score(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X_dev, y_dev, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### try prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X_dev,y_dev)\n",
    "desig_predicted=clf.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import collections, numpy\n",
    "prediction=le.inverse_transform(desig_predicted)\n",
    "c=collections.Counter(prediction)\n",
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    " #we insert prediction in index desig predict              \n",
    "df.loc[index_desig_predict,'designation']=prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quality=df['quality']\n",
    "df.drop(labels=['country','winery','variety','description','designation','quality','region_2'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tresh=df_dev.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>tf_winery</th>\n",
       "      <th>tf_variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alsace</td>\n",
       "      <td>cremant d'alsace</td>\n",
       "      <td>lucien albrecht</td>\n",
       "      <td>pinot blanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california</td>\n",
       "      <td>paso robles</td>\n",
       "      <td>castle rock</td>\n",
       "      <td>cabernet sauvignon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oregon</td>\n",
       "      <td>willamette valley</td>\n",
       "      <td>chateau bianca</td>\n",
       "      <td>gewurztraminer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alentejano</td>\n",
       "      <td>NaN</td>\n",
       "      <td>herdade do esporao</td>\n",
       "      <td>touriga nacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>southern italy</td>\n",
       "      <td>pompeiano</td>\n",
       "      <td>sorrentino</td>\n",
       "      <td>coda di volpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115186</th>\n",
       "      <td>california</td>\n",
       "      <td>napa valley</td>\n",
       "      <td>lail</td>\n",
       "      <td>bordeaux-style red blend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115187</th>\n",
       "      <td>california</td>\n",
       "      <td>dry creek valley</td>\n",
       "      <td>mounts</td>\n",
       "      <td>cabernet franc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115188</th>\n",
       "      <td>california</td>\n",
       "      <td>santa barbara county</td>\n",
       "      <td>tercero</td>\n",
       "      <td>g-s-m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115189</th>\n",
       "      <td>polkadraai hills</td>\n",
       "      <td>NaN</td>\n",
       "      <td>stellenbosch hills</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115190</th>\n",
       "      <td>california</td>\n",
       "      <td>santa lucia highlands</td>\n",
       "      <td>pessagno</td>\n",
       "      <td>riesling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115191 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                province               region_1           tf_winery  \\\n",
       "0                 alsace       cremant d'alsace     lucien albrecht   \n",
       "1             california            paso robles         castle rock   \n",
       "2                 oregon      willamette valley      chateau bianca   \n",
       "3             alentejano                    NaN  herdade do esporao   \n",
       "4         southern italy              pompeiano          sorrentino   \n",
       "...                  ...                    ...                 ...   \n",
       "115186        california            napa valley                lail   \n",
       "115187        california       dry creek valley              mounts   \n",
       "115188        california   santa barbara county             tercero   \n",
       "115189  polkadraai hills                    NaN  stellenbosch hills   \n",
       "115190        california  santa lucia highlands            pessagno   \n",
       "\n",
       "                      tf_variety  \n",
       "0                    pinot blanc  \n",
       "1             cabernet sauvignon  \n",
       "2                 gewurztraminer  \n",
       "3               touriga nacional  \n",
       "4                  coda di volpe  \n",
       "...                          ...  \n",
       "115186  bordeaux-style red blend  \n",
       "115187            cabernet franc  \n",
       "115188                     g-s-m  \n",
       "115189                       NaN  \n",
       "115190                  riesling  \n",
       "\n",
       "[115191 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1h=pd.get_dummies(df,sparse=True)   #one hot encoding of the categorical attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1h=hstack([df_1h,desig_words_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1h=df_1h.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev=df_1h[:tresh,:]\n",
    "y_dev=df_quality[:tresh]\n",
    "\n",
    "X_eval=df_1h[tresh:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115191, 17940)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### features reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "discarded approach; got only worse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=200, random_state=42)\n",
    "df_1h = svd.fit_transform(X_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(svd.explained_variance_ratio_,marker='o', linestyle='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## models evaluation and search:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "various models will be tested in order to find the best performer on the scoreboard. both old-out validation and cross validation were performed when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold out:\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_dev,y_dev, test_size= 0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) linear svr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 7, 'epsilon': 3, 'fit_intercept': True, 'max_iter': 1500}\n"
     ]
    }
   ],
   "source": [
    "reg = LinearSVR()\n",
    "param_grid = {'epsilon':[3,5], 'C':[4,5,6,7], 'fit_intercept':[True,False],'max_iter':[1500,2000]}\n",
    "gridsearch = GridSearchCV(reg, param_grid, scoring='r2', cv=5)\n",
    "\n",
    "gridsearch.fit(X_dev, y_dev)\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.74516161 0.71376035 0.71337893 0.74137182 0.72481427 0.72683652\n",
      " 0.7177324  0.71077976 0.73073225 0.72308581 0.71428657 0.72991698\n",
      " 0.72912269 0.72569897 0.71608954 0.72275414 0.70854414 0.7357361\n",
      " 0.71484268 0.73223929]\n"
     ]
    }
   ],
   "source": [
    "reg = LinearSVR(epsilon=3, C=6, max_iter=5000)\n",
    "scores_lsvr = cross_val_score(reg, X_dev, y_dev, cv=20, scoring='r2')\n",
    "print(scores_lsvr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009966621807733689"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_lsvr.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hold-out validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearSVR(epsilon=3, C=6, max_iter=5000)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_valid)\n",
    "\n",
    "r2 = r2_score(y_valid, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) ridge regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1, 'max_iter': 1000, 'solver': 'sparse_cg'}\n"
     ]
    }
   ],
   "source": [
    "reg = Ridge()\n",
    "param_grid = {'alpha':[0.5,1,10], 'max_iter':[1000,2000,5000], 'solver':['sparse_cg','sag']}\n",
    "gridsearch = GridSearchCV(reg, param_grid, scoring='r2', cv=5)\n",
    "\n",
    "gridsearch.fit(X_dev, y_dev)\n",
    "print(gridsearch.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=1,solver='sparse_cg',max_iter=1000)\n",
    "scores_ridge= cross_val_score(reg, X_dev, y_dev, cv=20, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75395561, 0.7198657 , 0.71942999, 0.74749069, 0.7336794 ,\n",
       "       0.73563732, 0.72429293, 0.71913457, 0.74277626, 0.73056684,\n",
       "       0.72159704, 0.73674058, 0.73962034, 0.7317529 , 0.73076569,\n",
       "       0.73087937, 0.71889236, 0.74497876, 0.7237664 , 0.74127446])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ridge.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hold-out validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=1,solver='sparse_cg',max_iter=1000)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred=reg.predict(X_valid)\n",
    "r2 = r2_score(y_valid, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) random forest regressor (feasible only when target encoding is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = RandomForestRegressor(n_estimators=1000, max_depth=30, max_features='sqrt')\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_valid)\n",
    "r2 = r2_score(y_valid, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) mlp regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "several mlp config were tested, here we only report few of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(10,10,10,64),\n",
    "                                learning_rate='adaptive',\n",
    "                                learning_rate_init=0.1,\n",
    "                                max_iter=10000,\n",
    "                                random_state=42,\n",
    "                                verbose=True,\n",
    "                                early_stopping=True,\n",
    "                   n_iter_no_change=20,\n",
    "                   tol=0.0001\n",
    "                            \n",
    "                \n",
    "                               )\n",
    "reg.fit(X_dev, y_dev)\n",
    "y_pred = reg.predict(X_eval)\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(64,128,128,516,516),\n",
    "                                random_state=42,\n",
    "                                verbose=True,\n",
    "                                early_stopping=True\n",
    "                               )\n",
    "                   \n",
    "reg.fit(X_dev, y_dev)\n",
    "y_pred = reg.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(64,128,128,128,256,256,256,516,516,516,1024),\n",
    "                                random_state=42,\n",
    "                                verbose=True,\n",
    "                                early_stopping=True\n",
    "                               )\n",
    "                   \n",
    "reg.fit(X_dev, y_dev)\n",
    "y_pred = reg.predict(X_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final models attempt (and print out):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearSVR(epsilon=3, C=6, max_iter=1500)\n",
    "reg.fit(X_dev, y_dev)\n",
    "y_pred = reg.predict(X_eval)\n",
    "\n",
    "pd.DataFrame(y_pred).to_csv(\"output_svr.csv\",index_label=\"Id\", header=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=1,solver='sparse_cg',max_iter=1000)\n",
    "reg.fit(X_dev, y_dev)\n",
    "\n",
    "y_pred = reg.predict(X_eval)\n",
    "pd.DataFrame(y_pred).to_csv(\"output_ridge.csv\",index_label=\"Id\", header=[\"Predicted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [8] best performer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = MLPRegressor(hidden_layer_sizes=(64,128,128,128,256,256,256,516,516,516,1024),\n",
    "                                random_state=42,\n",
    "                                verbose=True,\n",
    "                                early_stopping=True\n",
    "                               )\n",
    "                   \n",
    "reg.fit(X_dev, y_dev)\n",
    "y_pred = reg.predict(X_eval)\n",
    "\n",
    "pd.DataFrame(y_pred).to_csv(\"output_mlp.csv\",index_label=\"Id\", header=[\"Predicted\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
